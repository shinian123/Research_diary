一个agent，在黑暗中，对周围的state space一无所知，只能通过try-and-error去获取生存策略（policy），了解环境（environment）。它有两种方式，第一种方式不对环境进行研究，只研究policy和state的关系。它通过每次的尝试，来总结在某一个state，如果采取相应的动作（a），将来会得到的总收益（q（s，a））。当然这个估计一开始是不准确的，因此agent需要每次动态刷新这个收益值。每次刷新之后，agent就会根据当前的值函数去选取最优策略，即选取使值函数最大的action。这是model-free的方法。还有一种方式需要对环境进行研究，即agent先对发现它在当前的s0，如果采取某一动作a1，就会转移到s1，而若采取动作a2，就会转移到另一个状态s2，它发现这其中有个对应关系。于是它转而先研究这个对应关系，然后根据研究的结果来选择策略。如果它的起始状态是s0，而它想到sg，则agent会在simulator中构造出一种从s0到sg的路径。agent不需要亲自到sg，而是根据采样的轨迹就可以推知st，at，st+1的关系。
